# -*- coding: utf-8 -*-
"""NOIS case.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WX8sTWSwB6oOCXD_ahwPeEoEf-IRlMCx
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
!pip install scikit-plot
# %load_ext autotime

from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import datetime
import numpy as np
import matplotlib.mlab as mlab
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
import math,os
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
import scikitplot as skplt

#UPLOAD ZIPPED DATASET
files.upload()

#UNZIP DATASET
!unzip 'Maskinl√¶ring Oppgave 02 Data.zip'
!ls

#DATAFRAME CREATION
data = pd.read_csv('TrainAndValid.csv', sep=",")
data.head()

#FEATURE CREATION OF SALEDATE FEATURE
data['saledate'] = pd.to_datetime(data['saledate']) #Converting date to pandas datetime object
data['saledate_year'] = data['saledate'].dt.year
data['saledate_month'] = data['saledate'].dt.month
data['saledate_week'] = data['saledate'].dt.week
data['saledate_day'] = data['saledate'].dt.day
data['saledate_daysofweek'] = data['saledate'].dt.dayofweek
data.head()

#DATA STATISTICS
data.describe()
#SalePrice:
max_price = data['SalePrice'].max()
min_price = data['SalePrice'].min()
mean_price = data['SalePrice'].mean()
print("SalePrice statistics:\nMaximum price: " + "%.1f" % max_price + "\nMinimum price: " + "%.1f" % min_price + "\nMean price: " + "%.1f" % mean_price)

#Sale period:
first_sale = data['saledate'].min()
last_sale = data['saledate'].max()
print("Construction machinery was sold from " + str(first_sale.date()) + " to " + str(last_sale.date()) + ".")

#Sale numbers in histogram:
plt.figure()
plt.hist(data['saledate'].apply(lambda x: x.year))
plt.xlabel('Year')
plt.ylabel('Number of sales')
plt.title('Histogram over sale numbers')
plt.show()

#Price development:
nr_sale_years = int(last_sale.year)-int(first_sale.year)+1
sale_years = np.linspace(int(first_sale.year),int(last_sale.year),nr_sale_years)
sale_numbers = np.zeros(len(sale_years))
sale_sum = np.zeros(len(sale_years))

for index,row in data.iterrows():
  for i in range(0,len(sale_years)):
    if sale_years[i] == row['saledate'].year:
      sale_numbers[i] += 1
      sale_sum[i] += row['SalePrice']
      break
mean_price = np.zeros(len(sale_years))
for i in range(0,len(sale_years)):
  mean_price[i] = sale_sum[i] / sale_numbers[i]
coef = np.polyfit(sale_years, mean_price,1)
poly1d_fn = np.poly1d(coef) 
plt.figure()
plt.plot(sale_years, mean_price, sale_years, poly1d_fn(sale_years), '--r')
plt.xlabel('Year')
plt.ylabel('Mean price')
plt.title('Mean price development')
plt.show()

data.describe(include='all')

#ROWS WITH MISSING VALUES
print("Original data shape:\n" + str(data.shape))
data[data==np.inf]=np.nan #replaces inf with nan
print("Rows with missing values:\n " + str(data.isnull().sum()))

#VISUALIZING NUMBER OF MISSING VALUE PER FEATURE
fig, ax = plt.subplots(figsize=(20,5))
ax = (data.isnull().sum()).plot(kind='bar', title='Number of nans per feature')
ax.set_ylabel('Number of nan values')

fig2, ax2 = plt.subplots(figsize=(20,5))
ax2 = (data.isnull().sum()/data.shape[0]*100).plot(kind='bar', title='% of nans per feature')
ax2.set_ylabel('% nan values')

#REMOVING DUPLICATES
data.drop_duplicates(keep="first",inplace=True)

#REMOVAL OF ROWS WITH MISSING VALUES
data_1 = data.copy()
print(str(data_1.isnull().sum().max()) + " has been removed from the dataset.")
data_1.dropna(inplace=True) 
print("The new dataset size is :\n" + str(data_1.shape))
#No remaining rows; method not applicable

#IMPUTATION OF MISSING VALUES - MOST COMMON CATEGORY
data['UsageBand']= data['UsageBand'].fillna(data['UsageBand'].value_counts().index[0])
data['ProductSize']=data['ProductSize'].fillna(data['ProductSize'].value_counts().index[0])
data.head()

#IMPUTATION OF MISSING VALUES - MEAN VALUE
data['MachineHoursCurrentMeter']= data['MachineHoursCurrentMeter'].fillna(data['MachineHoursCurrentMeter'].mean())
data.head()
#consider performing grid search with median

#IMPUTATION OF MISSING VALUES - 0 
data['fiBaseModel']= data['fiBaseModel'].fillna(0)
data['fiSecondaryDesc']= data['fiSecondaryDesc'].fillna(str(0))
data['fiModelSeries']= data['fiModelSeries'].fillna(str(0))
data['fiModelDescriptor']= data['fiModelDescriptor'].fillna(str(0))
data.head()

#LABELING OF CATEGORY
data['state'] = LabelEncoder().fit_transform(data['state'])
data['ProductGroup'] = LabelEncoder().fit_transform(data['ProductGroup'])
data['ProductGroupDesc'] = LabelEncoder().fit_transform(data['ProductGroupDesc'])
data['fiProductClassDesc'] = LabelEncoder().fit_transform(data['fiProductClassDesc'])
data['ProductSize'] = LabelEncoder().fit_transform(data['ProductSize'])
data['UsageBand'] = LabelEncoder().fit_transform(data['UsageBand'])
data.head()

def correlation_heatmap(train):
    correlations = train_X.corr()

    fig, ax = plt.subplots(figsize=(10,10))
    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',
                square=True, linewidths=.5, annot=True, cbar_kws={"shrink": .70})
    plt.show();

#FEATURE CORRELATION
Y_col = ['SalePrice']
X = data.drop(Y_col, axis=1) #Input dataframe, X
Y = pd.DataFrame(data, columns=Y_col) #Output dataframe, Y

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, 
                                                    train_size=0.8,
                                                    test_size=0.2,
                                                    random_state=42)
correlation_heatmap(train_X)

#CHECKING HOW THE MEAN IS AFFECTED BY DROPPING NAN IN 'AUCTIONID'
data_dropped = data.dropna(subset=['auctioneerID']) 
# change in means as a bar chart
ax = ((data_dropped.mean() - data.mean()) / data.mean()).plot(kind='bar', title='% change in average column values')
ax.set_ylabel('% change')

#REMOVING CONFIGURATION COLUMNS
configuration_columns = ["Drive_System","Enclosure","Forks","Pad_Type","Ride_Control","Stick","Transmission","Turbocharged","Blade_Extension","Blade_Width","Enclosure_Type","Engine_Horsepower","Hydraulics","Pushblock","Ripper","Scarifier","Tip_Control","Tire_Size","Coupler","Coupler_System","Grouser_Tracks","Hydraulics_Flow","Track_Type","Undercarriage_Pad_Width","Stick_Length","Thumb","Pattern_Changer","Grouser_Type","Backhoe_Mounting","Blade_Type","Travel_Controls","Differential_Type","Steering_Controls"]
model_desc_columns = ["fiModelDesc","fiBaseModel","fiSecondaryDesc","fiModelSeries","fiModelDescriptor"]
sale_date_column = ["saledate"]
data_2 = data.copy()
data_2 = data_2.drop(configuration_columns, axis=1) 
data_2 = data_2.drop(model_desc_columns, axis=1) 
data_2 = data_2.drop(sale_date_column, axis=1) 
data_2.head()

print("Rows with missing values:\n " + str(data_2.isnull().sum()))

data_2.dropna(inplace=True) #remove rows with nan

#FEATURE CORRELATION
Y_col = ['SalePrice']
X = data_2.drop(Y_col, axis=1) #Input dataframe, X
Y = pd.DataFrame(data_2, columns=Y_col) #Output dataframe, Y

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, 
                                                    train_size=0.8,
                                                    test_size=0.2,
                                                    random_state=42)

#EXTRA TREES
et_500 = ExtraTreesRegressor(n_estimators=20, n_jobs=-1, random_state=300)
et_500.fit(train_X,train_Y)
predicted = et_500.predict(test_X)
scores = cross_val_score(et_500, X, Y, cv=5, scoring='r2')

def r2(y_true,y_pred):
    return roundup(r2_score(y_true,y_pred))

def roundup(a, digits=4):
    n = 10**-digits
    return round(math.ceil(a / n) * n, digits)

print(scores)

featureDisplay = data_2.columns
nr_features = data_2.shape[1]
def plot_feature_importances(et):
    skplt.estimators.plot_feature_importances(et,text_fontsize=16,max_num_features=nr_features,figsize=(24,4),feature_names=featureDisplay)
plot_feature_importances(et_500)
plt.xticks(rotation=90, fontsize = 20)
plt.yticks(fontsize = 20)
plt.title('Feature importance', fontsize = 30)
plt.show()

#REMOVING CORRELATING COLUMNS
correlating_columns = ["SalesID","ProductGroupDesc","saledate_week","fiProductClassDesc"]
data_3 = data_2.copy()
data_3 = data_3.drop(correlating_columns, axis=1) 
data_3.head()

#FEATURE CORRELATION
Y_col = ['SalePrice']
X = data_3.drop(Y_col, axis=1) #Input dataframe, X
Y = pd.DataFrame(data_3, columns=Y_col) #Output dataframe, Y

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, 
                                                    train_size=0.8,
                                                    test_size=0.2,
                                                    random_state=42)

#EXTRA TREES
from sklearn.ensemble import ExtraTreesRegressor
et_500 = ExtraTreesRegressor(n_estimators=20, n_jobs=-1, random_state=300)
et_500.fit(train_X,train_Y)
predicted = et_500.predict(test_X)
scores = cross_val_score(et_500, X, Y, cv=5, scoring='r2')

print(scores)

#REMOVING CONFIGURATION COLUMNS
correlating_columns = ["SalesID","ProductGroupDesc","saledate_week"]
data_4 = data_2.copy()
data_4 = data_4.drop(correlating_columns, axis=1) 
data_4.head()

#FEATURE CORRELATION
Y_col = ['SalePrice']
X = data_4.drop(Y_col, axis=1) #Input dataframe, X
Y = pd.DataFrame(data_4, columns=Y_col) #Output dataframe, Y

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, 
                                                    train_size=0.8,
                                                    test_size=0.2,
                                                    random_state=42)

#EXTRA TREES
et_500 = ExtraTreesRegressor(n_estimators=20, n_jobs=-1, random_state=300)
et_500.fit(train_X,train_Y)
predicted = et_500.predict(test_X)
scores = cross_val_score(et_500, X, Y, cv=5, scoring='r2')

print(scores)

#REMOVING CONFIGURATION COLUMNS
correlating_columns = ["SalesID","ProductGroupDesc","saledate_week", "datasource", "ProductGroup"]
data_5 = data_2.copy()
data_5 = data_5.drop(correlating_columns, axis=1) 
data_5.head()

#FEATURE CORRELATION
Y_col = ['SalePrice']
X = data_5.drop(Y_col, axis=1) #Input dataframe, X
Y = pd.DataFrame(data_3, columns=Y_col) #Output dataframe, Y

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, 
                                                    train_size=0.8,
                                                    test_size=0.2,
                                                    random_state=42)

#EXTRA TREES
et_500 = ExtraTreesRegressor(n_estimators=20, n_jobs=-1, random_state=300)
et_500.fit(train_X,train_Y)
predicted = et_500.predict(test_X)

scores = cross_val_score(et_500, X, Y, cv=5, scoring='r2')

print(scores)

#GRID SEARCH OF EXTRATREE SIZE
Y_col = ['SalePrice']
X = data_4.drop(Y_col, axis=1) #Input dataframe, X
Y = pd.DataFrame(data_4, columns=Y_col) #Output dataframe, Y

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, 
                                                    train_size=0.8,
                                                    test_size=0.2,
                                                    random_state=42)
# grid search
et_500 = ExtraTreesRegressor()
n_estimators = range(5,100,10)
param_grid = dict(n_estimators=n_estimators)
grid_search = GridSearchCV(et_500, param_grid, scoring="r2", n_jobs=-1, cv=5)
grid_result = grid_search.fit(train_X, train_Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
fit_time = grid_result.cv_results_['mean_fit_time']
score_time = grid_result.cv_results_['mean_score_time']
for mean, stdev, param, f_t,s_t in zip(means, stds, params, fit_time,score_time):
	print("%f (%f) with: %r, fit time: %f, score time: %f" % (mean, stdev, param,f_t,s_t))
# plot accuracy
plt.errorbar(n_estimators, means, yerr=stds)
plt.title("Extratrees n_estimators vs R2")
plt.xlabel('n_estimators')
plt.ylabel('R2')
plt.savefig('n_estimators.png')

# plot time
fig, host = plt.subplots()
fig.subplots_adjust(right=0.75)

par1 = host.twinx()

p1, = host.plot(n_estimators, fit_time, "b-", label="Fit time")
p2, = par1.plot(n_estimators, score_time, "r-", label="Score time")

host.set_xlabel('n_estimators')
host.set_ylabel("Fit time")
par1.set_ylabel("Score time")

host.yaxis.label.set_color(p1.get_color())
par1.yaxis.label.set_color(p2.get_color())

tkw = dict(size=4, width=1.5)
host.tick_params(axis='y', colors=p1.get_color(), **tkw)
par1.tick_params(axis='y', colors=p2.get_color(), **tkw)
host.tick_params(axis='x', **tkw)


lines = [p1, p2]

host.legend(lines, [l.get_label() for l in lines])

plt.show()

# PERFORMANCE:
 print("Best Accuracy: {}".format(grid_result.best_score_))
 # the best parameters that caused the best accuracy
 print("Best Parameters: {}".format(grid_result.best_params_))
 # the average time it took a model to fit to the data (in seconds)
 print("Average Time to Fit (s):{}".format(round(grid_result.cv_results_['mean_fit_time'].mean(), 3)))
 # the average time it took a model to predict out of sample data (in seconds)
 print("Average Time to Score (s):{}".format(round(grid_result.cv_results_['mean_score_time'].mean(), 3)))